{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# **Neural networks with SKlearn**\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Estimated time needed: **30** minutes\n","\n","In this lab, we will be implementing neural networks for a real-world task (ditgit recognition) using the **Multi-layer Perceptron (MLP)** classifier from scikit-learn.\n","\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Table of Contents\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","        </ol>       \n","    </li>\n","    <li><a href=\"https://#Background\">Background</a></li>\n","    <li><a href=\"#Example: Digit Recognition with Multi-layer Perceptron\">Example: Digit Recognition with Multi-layer Perceptron</a></li>\n","    \n","</ol>\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Apply MLP for classification tasks\n","*   Use RandomizedSearchCV to search for an optimal set of model parameters  \n"]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`Pillow`](https://pillow.readthedocs.io/en/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for image processing functions.\n","*   [`OpenCV`](https://docs.opencv.org/4.x/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for other image processing functions.\n","*   [`tensorflow`](https://www.tensorflow.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and neural network related functions.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook command in a different Jupyter environment (like Watson Studio or Anaconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62\n","\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n","\n","# RESTART YOUR KERNEL AFTERWARD AS WELL"]},{"cell_type":"markdown","metadata":{},"source":["### Importing Required Libraries\n","\n","*We recommend you import all required libraries in one place (here):*\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.simplefilter('ignore')\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.metrics import accuracy_score, classification_report\n"]},{"cell_type":"markdown","metadata":{},"source":["## Background\n"]},{"cell_type":"markdown","metadata":{},"source":["The scikit-learn library is well-known for providing robust and efficient tools for Machine Learning and Statistical Learning such as regression, classification, and clustering. It also contains an interface that allows us to work with neural networks, which is the **Multi-layer Perceptron (MLP)** class.\n","\n","A Multilayer Perceptron (MLP) is a fully connected class of feedforward artificial neural network (ANN). It consists of at least three layers of nodes: an input layer, a hidden layer, and an output layer. Except for the input layer, each layer contains nodes (neurons) that use nonlinear activation functions such as ReLu to learn complex and abstract features in the input.\n","\n","Class **MLPClassifier** utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish a MLP model from a linear model, as it can distinguish data that is not linearly separable.\n","\n","Note that Multilayer perceptrons are sometimes referred to as \"vanilla\" neural networks, especially when they have a single hidden layer. \n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/Artificial_neural_network.svg\" width=\"50%\"></center>\n","\n","<center>Illustration of One hidden layer MLP</center>\n"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["## Example: Digit Recognition with Multi-layer Perceptron\n","\n","In this example, you will implement a simple neural network using scikit-learn's **MLPClassifier** function. The goal is to correctly identify digits from a dataset of tens of thousands of handwritten images from [kaggle](https://www.kaggle.com/code/cezaryszulc/sklearn-simple-neural-network/notebook?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01).\n","\n","Let's download the digits dataset and display a few images!\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Import the data into a dataframe (with the help of the pandas library)\n","digits = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/digits.csv\")                                "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>pixel0</th>\n","      <th>pixel1</th>\n","      <th>pixel2</th>\n","      <th>pixel3</th>\n","      <th>pixel4</th>\n","      <th>pixel5</th>\n","      <th>pixel6</th>\n","      <th>pixel7</th>\n","      <th>pixel8</th>\n","      <th>...</th>\n","      <th>pixel774</th>\n","      <th>pixel775</th>\n","      <th>pixel776</th>\n","      <th>pixel777</th>\n","      <th>pixel778</th>\n","      <th>pixel779</th>\n","      <th>pixel780</th>\n","      <th>pixel781</th>\n","      <th>pixel782</th>\n","      <th>pixel783</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 785 columns</p>\n","</div>"],"text/plain":["   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n","0      1       0       0       0       0       0       0       0       0   \n","1      0       0       0       0       0       0       0       0       0   \n","2      1       0       0       0       0       0       0       0       0   \n","3      4       0       0       0       0       0       0       0       0   \n","4      0       0       0       0       0       0       0       0       0   \n","\n","   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n","0       0  ...         0         0         0         0         0         0   \n","1       0  ...         0         0         0         0         0         0   \n","2       0  ...         0         0         0         0         0         0   \n","3       0  ...         0         0         0         0         0         0   \n","4       0  ...         0         0         0         0         0         0   \n","\n","   pixel780  pixel781  pixel782  pixel783  \n","0         0         0         0         0  \n","1         0         0         0         0  \n","2         0         0         0         0  \n","3         0         0         0         0  \n","4         0         0         0         0  \n","\n","[5 rows x 785 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["digits.head()"]},{"cell_type":"markdown","metadata":{},"source":["From the command above, we now know that the input data includes a column 'label' and 784 colums that represent the intensity of each pixels in a 28x28 image"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["((42000, 784), (42000,))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["labels = digits['label']                                            # flat array of labels\n","digits = np.array(digits.drop('label', axis=1)).astype('float')     \n","digits.shape, labels.shape          "]},{"cell_type":"markdown","metadata":{},"source":["There are 42,000 digit images and each has 784 pixels, which means we can reshape them into $28\\times28$ images for displaying. In the code below, we are reshaping five random input samples to visualize the data\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x400 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.figure(figsize=(12,4))\n","for i in range(5):\n","    plt.subplot(1, 5, i+1)\n","    plt.imshow(random.choice(digits).reshape(28,28))\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's split the 42,000 images into train and test sets.\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["((29399, 784), (12601, 784))"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["split = 0.7, 0.3 # train, test\n","# normalize data over a range between 0 and 1\n","digits /= 255.0\n","\n","split_ind = int(len(digits)*split[0])\n","X_train, X_test, y_train, y_test = digits[:split_ind], digits[split_ind:], labels[:split_ind], labels[split_ind:]\n","X_train.shape, X_test.shape"]},{"cell_type":"markdown","metadata":{},"source":["With scikit-learn's **MLPClassifier**, we can utilize the GridSearch cross validation method to optimize the following parameters:\n","\n","- **hidden_layer_sizes: _tuple, length = n_layers - 2, default=(100,)_**. The ith element represents the number of neurons in the ith hidden layer.\n","\n","- **alpha: _float, default=0.0001_**. Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n","\n","- **max_iter: _int, default=200_**. Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n","\n","- **learning_rate_init: _float, default=0.001_**. The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n"]},{"cell_type":"markdown","metadata":{},"source":["Before we search for an optimal set of parameters, let's start with a vanilla MLPClassifier:\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using MLPClassifier with the default parameter values gives an accuracy of 0.9718276327275613\n"]}],"source":["model = MLPClassifier().fit(X_train, y_train)\n","y_pred = model.predict(X_test)\n","\n","print(f\"Using MLPClassifier with the default parameter values gives an accuracy of {accuracy_score(y_pred, y_test)}\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.98      0.98      0.98      1264\n","           1       0.99      0.98      0.99      1424\n","           2       0.97      0.98      0.97      1213\n","           3       0.96      0.98      0.97      1298\n","           4       0.97      0.97      0.97      1178\n","           5       0.97      0.96      0.96      1138\n","           6       0.99      0.98      0.98      1239\n","           7       0.98      0.97      0.97      1336\n","           8       0.97      0.95      0.96      1259\n","           9       0.95      0.96      0.95      1252\n","\n","    accuracy                           0.97     12601\n","   macro avg       0.97      0.97      0.97     12601\n","weighted avg       0.97      0.97      0.97     12601\n","\n"]}],"source":["print(classification_report(y_pred, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["For the cross validation training, we will use the default activation \"relu\" and default solver \"adam\". Using **RandomizedSearchCV** instead of **GridSearchCV** reduces the training time because in contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The following figure illustrates the difference between the two: \n","\n","<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module1/L1/data/randomsearch.png\" width=\"65%\"></center>\n","\n","<p style=\"text-align:center\">\n","<a href=\"https://dl.acm.org/doi/pdf/10.5555/2188385.2188395?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\"> Source: Random Search for Hyper-Parameter Optimization paper</a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{},"source":["As you can see, points in the GridSearch space are evenly distributed; thus projections onto either the important or the unimportant parameter subspace produces the same coverage, which is inefficient as we would want the important parameters to have more coverage in the trials. In contrast, the uneven distribution of points in the RandomSearch space allows the trials to explore many more distinct values in the important parameter space. \n","\n","Since it's hard to know ahead of time which parameter subspaces are more important, performing a parameter search on a strict, even grid would not be ideal or efficient.\n"]},{"cell_type":"markdown","metadata":{},"source":["However, despite the fact that we will use a more efficient search method since we are trying out multiple values for mutiple parameters of a neural network , **you should still expect training time to be much longer than training a single model**.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["The best parameter values found are:\n","\n","{'max_iter': 500, 'learning_rate_init': 0.001, 'hidden_layer_sizes': 200, 'alpha': 0.1}\n"]}],"source":["parameters = {'hidden_layer_sizes':[50, 100, 200],\n","              'alpha': [0.001, 0.01, 0.1], \n","              'max_iter': [200, 500, 800], \n","              'learning_rate_init':[0.0001, 0.001, 0.01, 0.1]}\n","\n","model = MLPClassifier()\n","clf = RandomizedSearchCV(estimator=model, param_distributions=parameters, cv=5)\n","clf.fit(X_train[:3000], y_train[:3000]) # reduce the train set size to shorten the training time\n","\n","print(\"The best parameter values found are:\\n\")\n","print(clf.best_params_)\n","\n","# store the best model found in \"bestmodel\"\n","bestmodel = clf.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["Now we can use the **bestmodel**, which uses the most optimal set of parameter values found by RandomSearchCV, cross-validated on a subset of the training set, to make predictions for the test set **X_test** and evaluate its performance.\n","\n","You select the best regularization  method using the validation data.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'bestmodel' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_691/3864156548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The accuracy score of the best model is {accuracy_score(y_test, y_pred)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'bestmodel' is not defined"]}],"source":["y_pred = bestmodel.predict(X_test)\n","print(f\"The accuracy score of the best model is {accuracy_score(y_test, y_pred)}\\n\")\n","\n","plt.figure(figsize=(12,8))\n","for i in range(10):\n","    plt.subplot(2, 5, i+1)\n","    sample = random.choice(X_test)\n","    plt.imshow(sample.reshape(28,28))\n","    pred = bestmodel.predict(sample.reshape(1,-1))\n","    plt.title(f\"Predicted as {pred}\")\n","    plt.axis(\"off\")\n","\n","plt.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy of our model is around 93% and the 10 random samples from the test set are all predicted correctly, which is pretty nice. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","metadata":{},"source":["[Roxanne Li](https://www.linkedin.com/in/roxanne-li/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01) is a Data Science intern at IBM Skills Network, entering level-5 study in the Mathematics & Statistics undergraduate Coop program at McMaster University.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By  | Change Description |\n","| ----------------- | ------- | ----------- | ------------------ |\n","| 2022-07-07        | 0.1     | Roxanne Li  | Created Lab       |\n","| 2022-09-06        | 0.1     | Steve Hord  | QA pass edits     |\n"]},{"cell_type":"markdown","metadata":{},"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
