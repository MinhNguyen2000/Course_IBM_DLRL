{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an environment with gym.make()\n",
    "env = gym.make('FrozenLake-v1',render_mode = \"human\") # Build a fresh environment\n",
    "\n",
    "# # Start a new game with env.reset()\n",
    "current_observation = env.reset() # This starts a new \"episode\" and returns the initial observation\n",
    "\n",
    "# # The current observation is just the current location\n",
    "print(current_observation) # Observations are just a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are fource discrete actions available in this environment\n",
    "print(f'Our action space: {env.action_space}')\n",
    "\n",
    "# Randomly sample an action and print the result\n",
    "new_action = env.action_space.sample()\n",
    "print(f'Our new action: {new_action}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_action = env.action_space.sample()\n",
    "\n",
    "observation, reward, term, trunc, info = env.step(new_action)\n",
    "\n",
    "print(f\"Action: {new_action}, observation: {observation}, reward: {reward}, terminated: {term}, truncated: {trunc}, info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = env.reset()\n",
    "\n",
    "term  = False\n",
    "for episode in range(100):\n",
    "    print(f'Episode {episode}')\n",
    "\n",
    "    while not term:\n",
    "        # Unpack and examine the result from taking a step in the environment\n",
    "        new_action = env.action_space.sample()\n",
    "        observation, reward, term, trunc, info = env.step(new_action)\n",
    "        print(f\"Action: {new_action}, observation: {observation}, reward: {reward}, terminated: {term}, truncated: {trunc}, info: {info}\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    term = False\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Gather Data\n",
    "In the last section, we have explored how to simulate episodes of the game from the start to a termination point. In this section, we would like to gather more of these episodes to start training an intelligent actor.\n",
    "\n",
    "We will let the system simulate 1000 episodes of the game and collect the data using pandas. Each row of the data frame is a step, while the features of each rows are\n",
    "\n",
    "- **`observation`** - the observation at the beginning of the step (before taking the action)\n",
    "- **`action`** - the randomly sampled action\n",
    "- **`DCF_reward`** - the distributed reward after each step leading to a final reward is taken. In other words, if the episode leads to the reward, current_reward is the final reward distributed to each step.\n",
    "- **`reward`** - the accumulated reward per episode. Subsequent to simulating every episode, this is used to calculate the success rate of reaching the goal\n",
    "\n",
    "Also consider a reward **`success`** that has a value of 1 at each step if the whole episode leads to final reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",render_mode = \"rgb_array\")\n",
    "\n",
    "# Intialize a list for storing the trainng data\n",
    "data = []  \n",
    "\n",
    "num_episodes = 40000\n",
    "for episode in range(num_episodes):\n",
    "    initial_observation = env.reset()[0]\n",
    "    done = False\n",
    "    cumsum_reward = 0 # Accumulated reward for each episode\n",
    "    step = 0\n",
    "    ep_data = []\n",
    "\n",
    "    while not(done):\n",
    "        # Sample a random action in the action space and take a step\n",
    "        new_action = env.action_space.sample()\n",
    "        new_observation, current_reward, term, trunc, info = env.step(new_action)\n",
    "\n",
    "        cumsum_reward += current_reward\n",
    "        step += 1\n",
    "        done = term or trunc\n",
    "\n",
    "        # Append a dictionary of key-value pairs to the list\n",
    "        ep_data.append({\n",
    "            # 'eps': episode,\n",
    "            'step': step,\n",
    "            'prevObs': initial_observation,\n",
    "            'act': new_action,\n",
    "            'currObs': new_observation,\n",
    "            'reward': current_reward,\n",
    "            'done': done\n",
    "        })\n",
    "\n",
    "        initial_observation = new_observation\n",
    "\n",
    "    # Adding a linearly discounted reward\n",
    "    num_steps = len(ep_data)\n",
    "    for idx, step in enumerate(ep_data):\n",
    "        step['success'] = cumsum_reward                         # To specify the steps that led to a reward\n",
    "        step[\"DCF_reward\"] = idx /num_steps * cumsum_reward     # The reward distributed evenly through the steps\n",
    "\n",
    "    data += ep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# df.describe()\n",
    "\n",
    "reward_per_eps = data_df['reward'].sum()/(episode+1)*100\n",
    "print(f\"\\n Success chance: {reward_per_eps}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Predict\n",
    "\n",
    "In this section, we are hoping to leverage the training data collected in the last example to improve the performance of our agent. We will adopt a supervised learning approach, where the input of the model is a combination of observation and action, and the model output is the expected reward of each of the possible action given a certain observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators = 50)\n",
    "# model = SVR()\n",
    "\n",
    "# The proposed structure of reward, weighting the three rewards accordingly\n",
    "    # The first component of the reward happens at goal states\n",
    "    # The second component of the rewards encourages actions that leads closer to the reward (since these states have higher DCF_reward)\n",
    "    # The third component of the reward encourages any action in a sequence in a successful episode\n",
    "y = 0.5 * data_df.reward + 0.1 * data_df.DCF_reward + data_df.success\n",
    "\n",
    "\n",
    "x = data_df[[\"prevObs\", \"act\"]]\n",
    "\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Act\n",
    "Now we have a model that predicts the desired behaviour, trained to estimate a reward value when given an observation and an action. We can modify the code used in the data gathering step so that we can replace random actions with more educated ones.\n",
    "\n",
    "With a random agent, we achieved a success rate of 1-2%. With more informed actions, the success rate should increase by 10x. \n",
    "\n",
    "How we can improve the model:\n",
    "- We can further tune the model for better performance. \n",
    "- Try different representations of the observation and actions spaces. \n",
    "- Try different models \n",
    "- Try different reward structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "y = 0.5 * data_df.reward + 0.1 * data_df.DCF_reward + data_df.success\n",
    "x = data_df[['prevObs','act']]\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "df.describe()\n",
    "\n",
    "reward_per_eps = data_df['reward'].sum()/(episode+1)*100\n",
    "print(f\"\\n Success chance: {reward_per_eps}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "data = []  \n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    initial_observation = env.reset()[0]\n",
    "    done = False\n",
    "    cumsum_reward = 0 # Accumulated reward for each episode\n",
    "    step = 0\n",
    "    ep_data = []\n",
    "\n",
    "    print(f\"Episode: {episode+1}\")\n",
    "\n",
    "    while not(done):\n",
    "        # Sample a random action in the action space and take a step\n",
    "        new_action = env.action_space.sample()\n",
    "\n",
    "        # # Make informed choice from the trained model\n",
    "        pred_in = pd.DataFrame([[initial_observation, i] for i in range(4)], columns = ['prevObs','act']) # This is the ['prevObs','act'] pair that is fed into the model\n",
    "        new_action = np.argmax(model.predict(pred_in))\n",
    "\n",
    "        # Take the step and record the result of the step\n",
    "        new_observation, current_reward, term, trunc, info = env.step(new_action)\n",
    "\n",
    "        cumsum_reward += current_reward\n",
    "        step += 1\n",
    "        done = term or trunc\n",
    "\n",
    "        # Append a dictionary of key-value pairs to the list\n",
    "        ep_data.append({\n",
    "            # 'eps': episode,\n",
    "            'step': step,\n",
    "            'prevObs': initial_observation,\n",
    "            'act': new_action,\n",
    "            'currObs': new_observation,\n",
    "            'reward': current_reward,\n",
    "            'done': done\n",
    "        })\n",
    "\n",
    "        initial_observation = new_observation\n",
    "\n",
    "    # Adding a linearly discounted reward\n",
    "    num_steps = len(ep_data)\n",
    "    for idx, step in enumerate(ep_data):\n",
    "        step['success'] = cumsum_reward                         # To specify the steps that led to a reward\n",
    "        step[\"DCF_reward\"] = idx /num_steps * cumsum_reward     # The reward distributed evenly through the steps\n",
    "\n",
    "    data += ep_data\n",
    "\n",
    "data_df = pd.DataFrame(data_df)\n",
    "data_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
