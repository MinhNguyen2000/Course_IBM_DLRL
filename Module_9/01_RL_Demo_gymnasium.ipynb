{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'prob': 1})\n"
     ]
    }
   ],
   "source": [
    "# Build an environment with gym.make()\n",
    "env = gym.make('FrozenLake-v1',render_mode = \"human\") # Build a fresh environment\n",
    "\n",
    "# # Start a new game with env.reset()\n",
    "current_observation = env.reset() # This starts a new \"episode\" and returns the initial observation\n",
    "\n",
    "# # The current observation is just the current location\n",
    "print(current_observation) # Observations are just a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our action space: Discrete(4)\n",
      "Our new action: 0\n"
     ]
    }
   ],
   "source": [
    "# There are fource discrete actions available in this environment\n",
    "print(f'Our action space: {env.action_space}')\n",
    "\n",
    "# Randomly sample an action and print the result\n",
    "new_action = env.action_space.sample()\n",
    "print(f'Our new action: {new_action}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, we will let the agent traverse through an entire episode in a random manner. \n",
    "\n",
    "An episode will end when a termination condition is met. A termination condition happens when the agent either falls into a hole or reaches the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 2, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 1, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 2, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 1, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 1, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 1, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 0, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 1, Action: 1, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 2, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 2, Action: 1, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 2, Action: 1, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 2, Action: 2, observation: 8, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 2, Action: 1, observation: 12, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 3, Action: 2, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 3, Action: 1, observation: 8, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 3, Action: 3, observation: 9, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 3, Action: 0, observation: 8, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 3, Action: 2, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 3, Action: 3, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 2, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 0, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 1, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 3, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 0, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 4, Action: 2, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 5, Action: 0, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 5, Action: 3, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 1, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 1, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 2, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 1, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 0, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 1, observation: 8, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 1, observation: 8, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 3, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 6, Action: 1, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 7, Action: 1, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 7, Action: 0, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 7, Action: 1, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 7, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 7, Action: 2, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 7, Action: 3, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 1, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 2, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 1, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 3, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 8, Action: 2, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 2, observation: 4, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 1, observation: 8, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 1, observation: 9, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 2, observation: 13, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 2, observation: 9, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 9, Action: 0, observation: 5, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 3, observation: 0, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 3, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 2, observation: 2, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 1, observation: 1, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 3, observation: 2, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 1, observation: 6, reward: 0.0, terminated: False, truncated: False, info: {'prob': 0.3333333333333333}\n",
      "Episode: 10, Action: 3, observation: 7, reward: 0.0, terminated: True, truncated: False, info: {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the testing environment and termination condition for every training episode\n",
    "    current_observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not(done):\n",
    "        # Sample a random action and take the action to observe the result\n",
    "        new_action = env.action_space.sample()\n",
    "        observation, reward, term, trunc, info = env.step(new_action)\n",
    "\n",
    "        # Print out the result of the action\n",
    "        print(f\"Episode: {episode+1}, Action: {new_action}, observation: {observation}, reward: {reward}, terminated: {term}, truncated: {trunc}, info: {info}\")\n",
    "\n",
    "        # Check the termination condition\n",
    "        done = term or trunc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Gather Data\n",
    "In the last section, we have explored how to simulate episodes of the game from the start to a termination point. In this section, we would like to gather more of these episodes to start training an intelligent actor.\n",
    "\n",
    "We will let the system simulate 1000 episodes of the game and collect the data using pandas. Each row of the data frame is a step, while the features of each rows are\n",
    "\n",
    "- **`observation`** - the observation at the beginning of the step (before taking the action)\n",
    "- **`action`** - the randomly sampled action\n",
    "- **`DCF_reward`** - the distributed reward after each step leading to a final reward is taken. In other words, if the episode leads to the reward, current_reward is the final reward distributed to each step.\n",
    "- **`reward`** - the accumulated reward per episode. Subsequent to simulating every episode, this is used to calculate the success rate of reaching the goal\n",
    "\n",
    "Also consider a reward **`success`** that has a value of 1 at each step if the whole episode leads to final reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Success chance: 1.3925%\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\",render_mode = \"rgb_array\")\n",
    "\n",
    "# Intialize a list for storing the trainng data\n",
    "train_data = []  \n",
    "\n",
    "num_episodes = 40000\n",
    "for episode in range(num_episodes):\n",
    "    initial_observation = env.reset()[0]\n",
    "    done = False\n",
    "    cumsum_reward = 0 # Accumulated reward for each episode\n",
    "    step = 0\n",
    "    ep_data = []\n",
    "\n",
    "    while not(done):\n",
    "        # Sample a random action in the action space and take a step\n",
    "        new_action = env.action_space.sample()\n",
    "        new_observation, current_reward, term, trunc, info = env.step(new_action)\n",
    "\n",
    "        cumsum_reward += current_reward\n",
    "        step += 1\n",
    "        done = term or trunc\n",
    "\n",
    "        # Append a dictionary of key-value pairs to the list\n",
    "        ep_data.append({\n",
    "            # 'eps': episode,\n",
    "            'step': step,\n",
    "            'prevObs': initial_observation,\n",
    "            'act': new_action,\n",
    "            'currObs': new_observation,\n",
    "            'reward': current_reward,\n",
    "            'done': done\n",
    "        })\n",
    "\n",
    "        initial_observation = new_observation\n",
    "\n",
    "    # Adding a linearly discounted reward\n",
    "    num_steps = len(ep_data)\n",
    "    for idx, step in enumerate(ep_data):\n",
    "        step['success'] = cumsum_reward                         # To specify the steps that led to a reward\n",
    "        step[\"DCF_reward\"] = idx /num_steps * cumsum_reward     # The reward distributed evenly through the steps\n",
    "\n",
    "    train_data += ep_data\n",
    "\n",
    "train_data_df = pd.DataFrame(train_data)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# df.describe()\n",
    "\n",
    "reward_per_eps = train_data_df['reward'].sum()/(episode+1)*100\n",
    "print(f\"\\n Success chance: {reward_per_eps}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Predict\n",
    "\n",
    "In this section, we are hoping to leverage the training data collected in the last example to improve the performance of our agent. We will adopt a supervised learning approach, where the input of the model is a combination of observation and action, and the model output is the expected reward of each of the possible action given a certain observation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesRegressor(n_estimators = 50)\n",
    "# model = SVR()\n",
    "\n",
    "# The proposed structure of reward, weighting the three rewards accordingly\n",
    "    # The first component of the reward happens at goal states\n",
    "    # The second component of the rewards encourages actions that leads closer to the reward (since these states have higher DCF_reward)\n",
    "    # The third component of the reward encourages any action in a sequence in a successful episode\n",
    "y = 0.5 * train_data_df.reward + 0.1 * train_data_df.DCF_reward + train_data_df.success\n",
    "\n",
    "\n",
    "x = train_data_df[[\"prevObs\", \"act\"]]\n",
    "\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Act\n",
    "Now we have a model that predicts the desired behaviour, trained to estimate a reward value when given an observation and an action. We can modify the code used in the data gathering step so that we can replace random actions with more educated ones.\n",
    "\n",
    "With a random agent, we achieved a success rate of 1-2%. With more informed actions, the success rate should increase by 10x. \n",
    "\n",
    "How we can improve the model:\n",
    "- We can further tune the model for better performance. \n",
    "- Try different representations of the observation and actions spaces. \n",
    "- Try different models \n",
    "- Try different reward structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "# model = ExtraTreesRegressor(n_estimators = 50)\n",
    "\n",
    "y = 0.5 * train_data_df.reward + 0.1 * train_data_df.DCF_reward + train_data_df.success\n",
    "x = train_data_df[['prevObs','act']]\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m new_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(model\u001b[38;5;241m.\u001b[39mpredict(pred_in))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Take the step and record the result of the step\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m new_observation, current_reward, term, trunc, info \u001b[38;5;241m=\u001b[39m \u001b[43menv_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m cumsum_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_reward\n\u001b[0;32m     31\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Minh Nguyen\\Course_IBM_DLRL\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Minh Nguyen\\Course_IBM_DLRL\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Minh Nguyen\\Course_IBM_DLRL\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Minh Nguyen\\Course_IBM_DLRL\\.venv\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:308\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p})\n",
      "File \u001b[1;32mc:\\Minh Nguyen\\Course_IBM_DLRL\\.venv\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Minh Nguyen\\Course_IBM_DLRL\\.venv\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:432\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    430\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m    431\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    435\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "test_data = []  \n",
    "\n",
    "# Create the testing environment. Set render_mode to rgb_array for faster testing, set render_mode to human for visualization (slower performance)\n",
    "env_test = gym.make(\"FrozenLake-v1\",render_mode = \"human\")\n",
    "# env_test = gym.make(\"FrozenLake-v1\",render_mode = \"rgb_array\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    initial_observation = env_test.reset()[0]\n",
    "    done = False\n",
    "    cumsum_reward = 0 # Accumulated reward for each episode\n",
    "    step = 0\n",
    "    ep_data = []\n",
    "\n",
    "    # print(f\"Episode: {episode+1}\")\n",
    "\n",
    "    while not(done):\n",
    "        # Sample a random action in the action space and take a step\n",
    "        new_action = env_test.action_space.sample()\n",
    "\n",
    "        # # Make informed choice from the trained model\n",
    "        pred_in = pd.DataFrame([[initial_observation, i] for i in range(4)], columns = ['prevObs','act']) # This is the ['prevObs','act'] pair that is fed into the model\n",
    "        new_action = np.argmax(model.predict(pred_in))\n",
    "\n",
    "        # Take the step and record the result of the step\n",
    "        new_observation, current_reward, term, trunc, info = env_test.step(new_action)\n",
    "\n",
    "        cumsum_reward += current_reward\n",
    "        step += 1\n",
    "        done = term or trunc\n",
    "\n",
    "        # Append a dictionary of key-value pairs to the list\n",
    "        ep_data.append({\n",
    "            # 'eps': episode,\n",
    "            'step': step,\n",
    "            'prevObs': initial_observation,\n",
    "            'act': new_action,\n",
    "            'currObs': new_observation,\n",
    "            'reward': current_reward,\n",
    "            'done': done\n",
    "        })\n",
    "\n",
    "        initial_observation = new_observation\n",
    "\n",
    "    # Adding a linearly discounted reward\n",
    "    num_steps = len(ep_data)\n",
    "    for idx, step in enumerate(ep_data):\n",
    "        step['success'] = cumsum_reward                         # To specify the steps that led to a reward\n",
    "        step[\"DCF_reward\"] = idx /num_steps * cumsum_reward     # The reward distributed evenly through the steps\n",
    "\n",
    "    test_data += ep_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Success chance: 61.386138613861384%\n"
     ]
    }
   ],
   "source": [
    "test_data_df = pd.DataFrame(test_data)\n",
    "\n",
    "reward_per_eps = test_data_df['reward'].sum()/(num_episodes+1)*100\n",
    "print(f\"\\n Success chance: {reward_per_eps}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
